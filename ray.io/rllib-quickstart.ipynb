{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c68adfe-c529-47b1-82c6-9c1aa52e4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06d0c1d-8480-404a-b4db-fd832c8b3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your problem using python and Farama-Foudation's gymnasium API:\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n",
    "    -----------------------\n",
    "    | S | 1 | 2 | 3 | G |\n",
    "    -----------------------\n",
    "    S = Start, G = goal, corridor_length = 5\n",
    "\n",
    "    Possible actions to chose from are: 0=left, 1=right\n",
    "    Observations are floats indicating the current field index, e.g. 0.0 for\n",
    "    starting position, 1.0 for the field next to the starting position, etc.\n",
    "    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Resets the episode.\n",
    "        Returns:\n",
    "            Initial observation of the new episode and an info dict.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos], {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given action.\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, terminatd-flag, truncated-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "\n",
    "        # Walk left\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Walk right\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "\n",
    "        # Set terminated flag when endo of corridor (goal) reached.\n",
    "        terminated = self.cur_pos >= self.end_pos\n",
    "        truncated = False\n",
    "\n",
    "        # +1 when goal reached, otherwise -1.\n",
    "        reward = 1.0 if terminated else -0.1\n",
    "        return [self.cur_pos], reward, terminated, truncated, {}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "111bde5c-f5b1-4597-af2e-17e97977d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tensorflow/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/opt/tensorflow/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/tensorflow/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/opt/tensorflow/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-24 04:36:18,884\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-24 04:36:25,615\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Create an RLlib Algorithm instance from a PPOConfig object.\n",
    "\n",
    "config = (\n",
    "    PPOConfig().environment(\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        env=SimpleCorridor,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        # Use corridor with 20 fields (including S and G).\n",
    "        env_config = {\"corridor_length\":28},\n",
    "    )\n",
    "    # Parallelize environment rollouts.\n",
    "    .env_runners(num_env_runners=3)\n",
    ")\n",
    "\n",
    "# Construct the actual (PPO) algorithm object from the config.\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742d544d-1573-4330-a22e-f262a95ef08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 04:36:37,400\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'episode_reward_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      7\u001b[0m     results \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; avg. reward=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Perform inference (action computations) based on given env observations.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Note that we are using a slightly different env here (len 10 instead of 20),\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# however, this should still work as the agent has (hopefully) learned\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# to \"just always walk right!\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m env \u001b[38;5;241m=\u001b[39m SimpleCorridor({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorridor_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m})\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode_reward_mean'"
     ]
    }
   ],
   "source": [
    "# Train for n iterations and report results (mean, episode rewards).\n",
    "# Since we have to move at least 19 times in the env to reach the goal and\n",
    "# each move gives us -0.1 reward (except the last move at the end: +1.0),\n",
    "# we can expect to reach an optimal episode reward of -0.1*18 + 1.0 = -0.8\n",
    "\n",
    "for i in range(10):\n",
    "    results = algo.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly different env here (len 10 instead of 20),\n",
    "# however, this should still work as the agent has (hopefully) learned\n",
    "# to \"just always walk right!\"\n",
    "\n",
    "env = SimpleCorridor({\"corridor_length\": 10})\n",
    "\n",
    "# Get the initial observation (should be: [0.0] for the starting position).\n",
    "obs, info = env.reset()\n",
    "terminated = truncated = False\n",
    "total_reward = 0.0\n",
    "\n",
    "# Play one episode.\n",
    "while not terminated and not truncated:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = algo.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "\n",
    "# Report results.\n",
    "print(f\"Played 1 episode; total-reward={total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
