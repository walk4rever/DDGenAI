{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c68adfe-c529-47b1-82c6-9c1aa52e4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c06d0c1d-8480-404a-b4db-fd832c8b3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your problem using python and Farama-Foudation's gymnasium API:\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Corridor in which an agent must learn to move right to reach the exit.\n",
    "    -----------------------\n",
    "    | S | 1 | 2 | 3 | G |\n",
    "    -----------------------\n",
    "    S = Start, G = goal, corridor_length = 5\n",
    "\n",
    "    Possible actions to chose from are: 0=left, 1=right\n",
    "    Observations are floats indicating the current field index, e.g. 0.0 for\n",
    "    starting position, 1.0 for the field next to the starting position, etc.\n",
    "    Rewards are -0.1 for all steps, except when reaching the goal (+1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Box(0.0, self.end_pos, shape=(1,))\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        \"\"\"Resets the episode.\n",
    "        Returns:\n",
    "            Initial observation of the new episode and an info dict.\n",
    "        \"\"\"\n",
    "\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos], {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Takes a single step in the episode given action.\n",
    "\n",
    "        Returns:\n",
    "            New observation, reward, terminatd-flag, truncated-flag, info-dict (empty).\n",
    "        \"\"\"\n",
    "\n",
    "        # Walk left\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        # Walk right\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "\n",
    "        # Set terminated flag when endo of corridor (goal) reached.\n",
    "        terminated = self.cur_pos >= self.end_pos\n",
    "        truncated = False\n",
    "\n",
    "        # +1 when goal reached, otherwise -1.\n",
    "        reward = 1.0 if terminated else -0.1\n",
    "        return [self.cur_pos], reward, terminated, truncated, {}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "111bde5c-f5b1-4597-af2e-17e97977d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yfzhu/opt/anaconda3/envs/py39/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:521: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/Users/yfzhu/opt/anaconda3/envs/py39/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/yfzhu/opt/anaconda3/envs/py39/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/Users/yfzhu/opt/anaconda3/envs/py39/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-16 17:11:36,084\tWARNING deprecation.py:50 -- DeprecationWarning: `WorkerSet(num_workers=... OR local_worker=...)` has been deprecated. Use `EnvRunnerGroup(num_env_runners=... AND local_env_runner=...)` instead. This will raise an error in the future!\n",
      "2024-05-16 17:11:36,085\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-16 17:11:45,374\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "2024-05-16 17:11:51,145\tINFO trainable.py:161 -- Trainable.setup took 15.063 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-16 17:11:51,146\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Create an RLlib Algorithm instance from a PPOConfig object.\n",
    "\n",
    "config = (\n",
    "    PPOConfig().environment(\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        env=SimpleCorridor,\n",
    "        # Config dict to be passed to our custom env's constructor.\n",
    "        # Use corridor with 20 fields (including S and G).\n",
    "        env_config = {\"corridor_length\":28},\n",
    "    )\n",
    "    # Parallelize environment rollouts.\n",
    "    .env_runners(num_env_runners=3)\n",
    ")\n",
    "\n",
    "# Construct the actual (PPO) algorithm object from the config.\n",
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742d544d-1573-4330-a22e-f262a95ef08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. reward=-2.5409090909090915\n",
      "Iter: 1; avg. reward=-2.2400000000000015\n",
      "Iter: 2; avg. reward=-2.174796747967481\n",
      "Iter: 3; avg. reward=-2.0362204724409465\n",
      "Iter: 4; avg. reward=-1.9113636363636377\n",
      "Iter: 5; avg. reward=-1.8742647058823543\n",
      "Iter: 6; avg. reward=-1.9045112781954898\n",
      "Iter: 7; avg. reward=-1.810218978102191\n",
      "Iter: 8; avg. reward=-1.7884892086330946\n",
      "Iter: 9; avg. reward=-1.7553956834532385\n",
      "Played 1 episode; total-reward=0.10000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Train for n iterations and report results (mean, episode rewards).\n",
    "# Since we have to move at least 19 times in the env to reach the goal and\n",
    "# each move gives us -0.1 reward (except the last move at the end: +1.0),\n",
    "# we can expect to reach an optimal episode reward of -0.1*18 + 1.0 = -0.8\n",
    "\n",
    "for i in range(10):\n",
    "    results = algo.train()\n",
    "    print(f\"Iter: {i}; avg. reward={results['episode_reward_mean']}\")\n",
    "\n",
    "# Perform inference (action computations) based on given env observations.\n",
    "# Note that we are using a slightly different env here (len 10 instead of 20),\n",
    "# however, this should still work as the agent has (hopefully) learned\n",
    "# to \"just always walk right!\"\n",
    "\n",
    "env = SimpleCorridor({\"corridor_length\": 10})\n",
    "\n",
    "# Get the initial observation (should be: [0.0] for the starting position).\n",
    "obs, info = env.reset()\n",
    "terminated = truncated = False\n",
    "total_reward = 0.0\n",
    "\n",
    "# Play one episode.\n",
    "while not terminated and not truncated:\n",
    "    # Compute a single action, given the current observation\n",
    "    # from the environment.\n",
    "    action = algo.compute_single_action(obs)\n",
    "    # Apply the computed action in the environment.\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # Sum up rewards for reporting purposes.\n",
    "    total_reward += reward\n",
    "\n",
    "# Report results.\n",
    "print(f\"Played 1 episode; total-reward={total_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
